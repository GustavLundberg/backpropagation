{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Make sure not to forget the biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation:\n",
    "    \n",
    "    def __init__(self, activation):\n",
    "        self.activation = activation\n",
    "        \n",
    "    def output_scalar(self, x):\n",
    "        if self.activation == 'sigmoid':\n",
    "            return 1/(1+np.exp(-x))\n",
    "        elif self.activation == 'relu':\n",
    "            return np.max(0, x)\n",
    "        else:\n",
    "            print('Activation function not yet defined')\n",
    "            return None\n",
    "        \n",
    "    def gradient_scalar(self, x):\n",
    "        if self.activation == 'sigmoid':\n",
    "            return x*(1-x)\n",
    "        elif self.activation == 'relu':\n",
    "            if x > 0:\n",
    "                return 1\n",
    "            else:\n",
    "                return 0\n",
    "        else:\n",
    "            print('Activation function not yet defined')\n",
    "            return None\n",
    "        \n",
    "    def output_vectorized(self, x):\n",
    "        if self.activation == 'sigmoid':\n",
    "            return 1/(1+np.exp(-x))\n",
    "        elif self.activation == 'relu':\n",
    "            return np.maximum(0, x)\n",
    "        elif self.activation == 'linear':\n",
    "            return x\n",
    "        else:\n",
    "            print('Activation function not yet defined')\n",
    "            return None\n",
    "        \n",
    "    def gradient_vectorized(self, x):\n",
    "        if self.activation == 'sigmoid':\n",
    "            return x*(1-x)\n",
    "        elif self.activation == 'relu':\n",
    "            return 1 * (x > 0)\n",
    "        elif self.activation == 'linear':\n",
    "            return np.ones(len(x))\n",
    "        else:\n",
    "            print('Activation function not yet defined')\n",
    "            return None\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    \n",
    "    def __init__(self, nodes_lst, input_shape, activations):\n",
    "        self.nodes_lst = nodes_lst\n",
    "        self.input_shape = input_shape\n",
    "        self.activations = [Activation(a) for a in activations] # List of Activation objects\n",
    "        self.w = self.initialize_weights(nodes_lst, input_shape)\n",
    "        \n",
    "    def initialize_weights(self, nodes_lst, input_shape):\n",
    "        w_lst = []\n",
    "        w_lst.append(np.random.normal(size = (input_shape + 1, nodes_lst[0]))) # + 1 because we are including the biases as the first row of the weights matrix for each layer\n",
    "        for x, y in zip(nodes_lst, nodes_lst[1:]):\n",
    "            w_lst.append(np.random.normal(size = (x + 1, y)))                  # Again, \"+ 1\" to account for the biases\n",
    "        \n",
    "        return w_lst\n",
    "    \n",
    "    def get_w_shapes(self):\n",
    "        return [w_layer.shape for w_layer in self.w]\n",
    "    \n",
    "    # Forward propagation through a single layer\n",
    "    # x - input to the current layer\n",
    "    def propagate_forward(self, x, w, activation_object):\n",
    "        #print('x: ', x.shape)\n",
    "        x = np.append(1, x) # To account for the biases; first element of the input to a layer needs to be a one\n",
    "        #print('x: ', x.shape)\n",
    "        #print('w: ', w.shape)\n",
    "        weighted_sum = np.dot(x, w)\n",
    "        output = activation_object.output_vectorized(weighted_sum)\n",
    "        \n",
    "        return weighted_sum, output\n",
    "    \n",
    "    # A \"full\" forward pass, from input layer to output layer\n",
    "    # x - input to the network (i.e. the first layer)\n",
    "    def forward_pass(self, x):\n",
    "        \n",
    "        weighted_sum_lst = []\n",
    "        output_lst = []\n",
    "        \n",
    "        for weights, activation in zip(self.w, self.activations):\n",
    "            weighted_sum, output = self.propagate_forward(x, weights, activation)\n",
    "            weighted_sum_lst.append(weighted_sum)\n",
    "            output_lst.append(output)\n",
    "            x = output # Output of this layer becomes input to next layer\n",
    "            \n",
    "        return weighted_sum_lst, output_lst\n",
    "    \n",
    "    # Calculating the gradients of the final layer\n",
    "    # x            - input to the final layer (vector)\n",
    "    # weighted_sum - the weighted sums of all the nodes in the final layer (vector)\n",
    "    # y            - the output of the final layer\n",
    "    # t            - the targets (vector)\n",
    "    # w            - the weights of the final layer (matrix) - THIS ONE DOES NOT SEEM TO BE NEEDED!\n",
    "    # activation   - the activation of the final layer (activation object)\n",
    "    def gradients_final_layer(self, x, weighted_sum, y, t, w, activation):\n",
    "        dE_dy = -(t - y)\n",
    "        dy_dweighted_sum = activation.gradient_vectorized(weighted_sum)\n",
    "        dweighted_sum_dw = np.append(1, x) # Accounting for the bias node\n",
    "#        print(dE_dy.shape)\n",
    "#        print(dy_dweighted_sum.shape)\n",
    "#        print(dweighted_sum_dw.shape)\n",
    "\n",
    "        delta = - np.reshape(dE_dy * dy_dweighted_sum, (1, -1))\n",
    "        dE_dw = np.dot(np.reshape(dweighted_sum_dw, (-1, 1)), delta)\n",
    "        \n",
    "        return delta, dE_dw\n",
    "    \n",
    "    # Calculating the gradients of a hidden layer\n",
    "    # x            - input to the hidden layer (vector)\n",
    "    # weighted_sum - the weighted sums of all the nodes in the hidden layer (vector)\n",
    "    # y            - the output of the hidden layer\n",
    "    # delta        - the delta from the layer after the hidden layer\n",
    "    # w_hidden     - the weights of the hidden layer (matrix) - THIS ONE DOES NOT SEEM TO BE NEEDED!\n",
    "    # w_next       - the weights of the next layer (matrix)\n",
    "    # activation   - the activation of the hidden layer (activation object)\n",
    "    def gradients_hidden_layer(self, x, weighted_sum, y, delta, w_next, activation):\n",
    "        #print('INSIDE w_next: ', w_next.shape)\n",
    "        #print('INSIDE delta: ', delta.shape)\n",
    "        w_next = w_next[1:]\n",
    "        #print('INSIDE w_next: ', w_next.shape)\n",
    "        dE_dy = - np.dot(w_next, np.reshape(delta, (-1, 1)))\n",
    "        dy_dweighted_sum = activation.gradient_vectorized(weighted_sum)\n",
    "        dweighted_sum_dw = np.reshape(np.append(1, x), (-1, 1))\n",
    "        \n",
    "        #print('INSIDE ', dE_dy.shape, dy_dweighted_sum.shape)\n",
    "        \n",
    "        delta = - np.reshape(np.reshape(dE_dy, (-1)) * dy_dweighted_sum, (1, -1))\n",
    "        #print('INSIDE dweighted_sum_dw: ', dweighted_sum_dw.shape)\n",
    "        #print('INSIDE delta: ', delta.shape)\n",
    "        dE_dw = np.dot(dweighted_sum_dw, delta)\n",
    "        \n",
    "        #print(dE_dy.shape)\n",
    "        #print(dy_dweighted_sum.shape)\n",
    "        #print(dweighted_sum_dw.shape)\n",
    "        \n",
    "        return delta, dE_dw\n",
    "        \n",
    "    # Compute all gradients (w.r.t. the weights) of the network\n",
    "    # x - input to the network\n",
    "    # t - targets\n",
    "    def compute_all_gradients(self, x, t):\n",
    "        weighted_sum_lst, output_lst = self.forward_pass(x)\n",
    "        output_lst = [x] + output_lst # Adding the input to the beginning of the output_lst for convenience\n",
    "        delta_lst = []\n",
    "        dE_dw_lst = []\n",
    "        \n",
    "        # Compute gradients of the final layer\n",
    "        delta, dE_dw = model.gradients_final_layer(x = output_lst[-2], \n",
    "                                                   weighted_sum = weighted_sum_lst[-1], \n",
    "                                                   y = output_lst[-1], \n",
    "                                                   t = t, \n",
    "                                                   w = model.w[-1], \n",
    "                                                   activation = model.activations[-1])\n",
    "        \n",
    "        delta_lst.append(delta)\n",
    "        dE_dw_lst.append(dE_dw)\n",
    "        \n",
    "        for i in range(1, len(weighted_sum_lst)):\n",
    "            print('i = ', i)\n",
    "            print('output_lst[-(i+1)] = ', output_lst[-(i+1)])\n",
    "            delta, dE_dw = model.gradients_hidden_layer(x = output_lst[-(i+2)], \n",
    "                                                        weighted_sum = weighted_sum_lst[-(i+1)], \n",
    "                                                        y = output_lst[-(i+1)], \n",
    "                                                        delta = delta_lst[-1], # The 'last' delta produced\n",
    "                                                        w_next = model.w[-i], \n",
    "                                                        activation = model.activations[-(i+1)])\n",
    "            \n",
    "            \n",
    "#            delta_hidden, dE_dw = model.gradients_hidden_layer(x = x, \n",
    " #                                     weighted_sum = weighted_sum_lst[-2], \n",
    "  #                                    y = output_lst[-2], \n",
    "   #                                   delta = delta_final,\n",
    "    #                                  w_next = model.w[-1], \n",
    "     #                                 activation = model.activations[-2])\n",
    "            \n",
    "            \n",
    "            \n",
    "            delta_lst.append(delta)\n",
    "            dE_dw_lst.append(dE_dw)\n",
    "            \n",
    "        delta_lst = list(reversed(delta_lst)) # Making sure an increases index means we're going from input layer to output layer\n",
    "        dE_dw_lst = list(reversed(dE_dw_lst)) # Making sure an increases index means we're going from input layer to output layer\n",
    "        \n",
    "        return delta_lst, dE_dw_lst\n",
    "\n",
    "        \n",
    "        \n",
    "    def update_weights(self, dE_dw_lst, learning_rate):\n",
    "        self.w = [w - learning_rate*dE_dw for w, dE_dw in zip(self.w, dE_dw_lst)]\n",
    "        return\n",
    "    \n",
    "    def train_model(self, x, t, num_epochs, learning_rate):\n",
    "        \n",
    "        history = []\n",
    "    \n",
    "        for ep in range(num_epochs):\n",
    "            _, output_lst = self.forward_pass(x)\n",
    "            output_final_layer = output_lst[-1]\n",
    "            error = np.dot(t-output_final_layer, t-output_final_layer)\n",
    "            history.append(error)\n",
    "                \n",
    "            delta_lst, dE_dw_lst = self.compute_all_gradients(x, t)\n",
    "            self.update_weights(dE_dw_lst, learning_rate)\n",
    "            \n",
    "        return history\n",
    "        \n",
    "            \n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([0, 1, 2]), array([0, 1, 2, 3, 4])]\n",
      "[0 1 2]\n",
      "[0 1 2 3 4]\n",
      "[array([0, 1, 2]), array([0, 1, 2, 3, 4])]\n"
     ]
    }
   ],
   "source": [
    "lst = [np.array(range(3)), np.array(range(5))]\n",
    "print(lst)\n",
    "for a in lst:\n",
    "    print(a)\n",
    "    a = a+1\n",
    "print(lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight shapes:  [(6, 4), (5, 3)]\n",
      "weigted_sum_lst:  2 (4,) (3,)\n",
      "output_lst:  2 (4,) (3,)\n"
     ]
    }
   ],
   "source": [
    "### Complete forward pass ###\n",
    "model = NeuralNetwork(nodes_lst = [4, 3], input_shape = 5, activations = ['sigmoid', 'relu'])\n",
    "print('weight shapes: ', model.get_w_shapes())\n",
    "x = np.array(list(range(5)))\n",
    "weighted_sum_lst, output_lst = model.forward_pass(x)\n",
    "print('weigted_sum_lst: ', len(weigted_sum_lst), weigted_sum_lst[0].shape, weigted_sum_lst[1].shape)\n",
    "print('output_lst: ', len(output_lst), output_lst[0].shape, output_lst[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "delta_final =  (1, 3)\n",
      "dE_dw =  (5, 3)\n",
      "This seems to be OK!\n"
     ]
    }
   ],
   "source": [
    "### Calculate gradients of the last layer ###\n",
    "t = weighted_sum_lst[-1] + 0.1 # Simulating the targets\n",
    "delta_final, dE_dw = model.gradients_final_layer(x = output_lst[-2], \n",
    "                                           weighted_sum = weighted_sum_lst[-1], \n",
    "                                           y = output_lst[-1], \n",
    "                                           t = t, \n",
    "                                           w = model.w[-1], \n",
    "                                           activation = model.activations[-1])\n",
    "print('delta_final = ', delta_final.shape)\n",
    "print('dE_dw = ', dE_dw.shape)\n",
    "print('This seems to be OK!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "delta_hidden =  (1, 4)\n",
      "dE_dw =  (6, 4)\n",
      "This seems to be OK!\n"
     ]
    }
   ],
   "source": [
    "### Calculate gradients of the last hidden layer ###\n",
    "x = np.array(list(range(5)))\n",
    "delta_hidden, dE_dw = model.gradients_hidden_layer(x = x, \n",
    "                                      weighted_sum = weighted_sum_lst[-2], \n",
    "                                      y = output_lst[-2], \n",
    "                                      delta = delta_final,\n",
    "                                      w_next = model.w[-1], \n",
    "                                      activation = model.activations[-2])\n",
    "\n",
    "print('delta_hidden = ', delta_hidden.shape)\n",
    "print('dE_dw = ', dE_dw.shape)\n",
    "print('This seems to be OK!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i =  1\n",
      "output_lst[-(i+1)] =  [0.29794241 0.98041342 0.00770862 0.97329948]\n",
      "delta_lst =  2 (1, 4) (1, 3)\n",
      "dE_dw_lst =  2 (6, 4) (5, 3)\n"
     ]
    }
   ],
   "source": [
    "### Compute all gradients ###\n",
    "x = np.array(list(range(5)))\n",
    "t = np.array(list(range(3)))+1\n",
    "delta_lst, dE_dw_lst = model.compute_all_gradients(x, t)\n",
    "print('delta_lst = ', len(delta_lst), delta_lst[0].shape, delta_lst[1].shape)\n",
    "print('dE_dw_lst = ', len(dE_dw_lst), dE_dw_lst[0].shape, dE_dw_lst[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w before:  [(6, 4), (5, 3)]\n",
      "[array([[0.66236278, 0.13185854, 0.93901224, 2.01088436],\n",
      "       [0.        , 0.        , 0.        , 0.        ],\n",
      "       [0.66236278, 0.13185854, 0.93901224, 2.01088436],\n",
      "       [1.32472557, 0.26371708, 1.87802449, 4.02176873],\n",
      "       [1.98708835, 0.39557562, 2.81703673, 6.03265309],\n",
      "       [2.64945114, 0.52743416, 3.75604897, 8.04353746]]), array([[ 0.        , -0.30986135,  0.        ],\n",
      "       [ 0.        , -0.09232084,  0.        ],\n",
      "       [ 0.        , -0.30379223,  0.        ],\n",
      "       [ 0.        , -0.0023886 ,  0.        ],\n",
      "       [ 0.        , -0.30158789,  0.        ]])] 2\n",
      "w after : [(6, 4), (5, 3)]\n"
     ]
    }
   ],
   "source": [
    "### Update weights ###\n",
    "print('w before: ', model.get_w_shapes())\n",
    "print(dE_dw_lst, len(dE_dw_lst))\n",
    "model.update_weights(dE_dw_lst = dE_dw_lst, learning_rate = 0.5)\n",
    "print('w after :', model.get_w_shapes())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i =  1\n",
      "output_lst[-(i+1)] =  [nan nan nan nan]\n",
      "i =  1\n",
      "output_lst[-(i+1)] =  [nan nan nan nan]\n",
      "i =  1\n",
      "output_lst[-(i+1)] =  [nan nan nan nan]\n",
      "[nan, nan, nan]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gustav.k.lundberg\\AppData\\Local\\Continuum\\anaconda3\\envs\\OpenAIGym\\lib\\site-packages\\ipykernel_launcher.py:42: RuntimeWarning: invalid value encountered in greater\n"
     ]
    }
   ],
   "source": [
    "x = np.array(list(range(5)))\n",
    "t = np.array(list(range(3)))+1\n",
    "history = model.train_model(x, t, num_epochs = 3, learning_rate = 0.001)\n",
    "print(history)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "OpenAIGym",
   "language": "python",
   "name": "openaigym"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
